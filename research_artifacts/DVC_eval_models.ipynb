{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "def load_data(data_path: Path):\n",
    "    df_train = pd.read_csv(data_path / \"train.csv\")\n",
    "    df_valid = pd.read_csv(data_path / \"val.csv\")\n",
    "    df_test = pd.read_csv(data_path / \"test.csv\")\n",
    "\n",
    "    X_train = df_train.drop(columns=[\"label\"])\n",
    "    y_train = df_train[\"label\"]\n",
    "    X_valid = df_valid.drop(columns=[\"label\"])\n",
    "    y_valid = df_valid[\"label\"]\n",
    "    X_test = df_test.drop(columns=[\"label\"])\n",
    "    y_test = df_test[\"label\"]\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "def load_model(model_path: Path):\n",
    "    try:\n",
    "        model = CatBoostClassifier()\n",
    "        model.load_model(model_path)\n",
    "        return model\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        model = xgb.XGBClassifier()\n",
    "        model.load_model(model_path)\n",
    "        return model\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        model = lgb.Booster(model_file=model_path)\n",
    "        return model\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(clf, X_test, y_test, metric):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = (y_pred > 0.5).astype(int)\n",
    "    else:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "    if metric == \"F1\":\n",
    "        return f1_score(y_test, y_pred, average=\"binary\")\n",
    "    elif metric == \"Accuracy\":\n",
    "        return accuracy_score(y_test, y_pred)\n",
    "    else:\n",
    "        return matthews_corrcoef(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dir = Path(\"../data/embeddings\")\n",
    "\n",
    "DATASETS = {\n",
    "    \"enhancers\": embeddings_dir / \"enhancers\",\n",
    "    \"promoter_all\": embeddings_dir / \"promoter_all\",\n",
    "    \"splice_sites_all\": embeddings_dir / \"splice_sites_all\",\n",
    "    \"H3K9me3\": embeddings_dir / \"H3K9me3\",\n",
    "    \"H4K20me1\": embeddings_dir / \"H4K20me1\",\n",
    "}\n",
    "\n",
    "METRICS = {\n",
    "    \"enhancers\": \"MCC\",\n",
    "    \"promoter_all\": \"F1\",\n",
    "    \"splice_sites_all\": \"Accuracy\",\n",
    "    \"H3K9me3\": \"MCC\",\n",
    "    \"H4K20me1\": \"MCC\",\n",
    "}\n",
    "\n",
    "save_model_dir = Path(\"../models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def evaluate_models():\n",
    "    results = {}\n",
    "    for dataset_name in DATASETS:\n",
    "        results[dataset_name] = {}\n",
    "        dataset_path = DATASETS[dataset_name]\n",
    "        metric = METRICS[dataset_name]\n",
    "        _, _, _, _, X_test, y_test = load_data(dataset_path)\n",
    "        models_dir = save_model_dir / dataset_name\n",
    "        for file in os.listdir(models_dir):\n",
    "            if file.endswith(\".pkl\"):\n",
    "                model_name = file.split(\".\")[0]\n",
    "                model_path = models_dir / file\n",
    "                model = load_model(model_path)\n",
    "                print(f\"Evaluating {model_name} on {dataset_name} with {metric} metric\")\n",
    "                score = evaluate_model(model, X_test, y_test, metric)\n",
    "                results[dataset_name][model_name] = score\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating xgboost on enhancers with MCC metric\n",
      "Evaluating catboost on enhancers with MCC metric\n",
      "Evaluating lightgbm on enhancers with MCC metric\n",
      "Evaluating xgboost on promoter_all with F1 metric\n",
      "Evaluating catboost on promoter_all with F1 metric\n",
      "Evaluating lightgbm on promoter_all with F1 metric\n",
      "Evaluating xgboost on splice_sites_all with Accuracy metric\n",
      "Evaluating catboost on splice_sites_all with Accuracy metric\n",
      "Evaluating lightgbm on splice_sites_all with Accuracy metric\n",
      "Evaluating xgboost on H3K9me3 with MCC metric\n",
      "Evaluating catboost on H3K9me3 with MCC metric\n",
      "Evaluating lightgbm on H3K9me3 with MCC metric\n",
      "Evaluating xgboost on H4K20me1 with MCC metric\n",
      "Evaluating catboost on H4K20me1 with MCC metric\n",
      "Evaluating lightgbm on H4K20me1 with MCC metric\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+------------+------------+\n",
      "| Dataset          |   XGBoost |   CatBoost |   LightGBM |\n",
      "+==================+===========+============+============+\n",
      "| enhancers        |    0.4721 |     0.4758 |     0.4638 |\n",
      "+------------------+-----------+------------+------------+\n",
      "| promoter_all     |    0.8508 |     0.85   |     0.8557 |\n",
      "+------------------+-----------+------------+------------+\n",
      "| splice_sites_all |    0.4807 |     0.343  |     0.5437 |\n",
      "+------------------+-----------+------------+------------+\n",
      "| H3K9me3          |    0.2801 |     0.2896 |     0.2894 |\n",
      "+------------------+-----------+------------+------------+\n",
      "| H4K20me1         |    0.5742 |     0.5832 |     0.5843 |\n",
      "+------------------+-----------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "headers = ['Dataset', 'XGBoost', 'CatBoost', 'LightGBM']\n",
    "table_data = []\n",
    "\n",
    "for dataset, models in results.items():\n",
    "    row = [\n",
    "        dataset,\n",
    "        f\"{models['xgboost']:.4f}\",\n",
    "        f\"{models['catboost']:.4f}\",\n",
    "        f\"{models['lightgbm']:.4f}\"\n",
    "    ]\n",
    "    table_data.append(row)\n",
    "\n",
    "print(tabulate(table_data, headers=headers, tablefmt='grid'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
